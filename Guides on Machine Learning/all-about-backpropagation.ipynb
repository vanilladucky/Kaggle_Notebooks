{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b61d7c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:49.545015Z",
     "iopub.status.busy": "2023-01-23T06:50:49.544386Z",
     "iopub.status.idle": "2023-01-23T06:50:49.568297Z",
     "shell.execute_reply": "2023-01-23T06:50:49.566881Z"
    },
    "papermill": {
     "duration": 0.048999,
     "end_time": "2023-01-23T06:50:49.571101",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.522102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/seeds-dataset/seeds_dataset.txt\n",
      "/kaggle/input/seed-from-uci/Seed_Data.csv\n",
      "/kaggle/input/spaceship-titanic/sample_submission.csv\n",
      "/kaggle/input/spaceship-titanic/train.csv\n",
      "/kaggle/input/spaceship-titanic/test.csv\n",
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908c751",
   "metadata": {
    "papermill": {
     "duration": 0.016654,
     "end_time": "2023-01-23T06:50:49.603411",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.586757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Welcome to All About Backpropagation ðŸ¥¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29317283",
   "metadata": {
    "papermill": {
     "duration": 0.015622,
     "end_time": "2023-01-23T06:50:49.637866",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.622244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color: lightblue;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:black;\">\n",
    "    Welcome back! If you are new here, I'm currently a computer science student, an avid number nerd, an aspiring researcher who publishes <a href=\"https://www.kaggle.com/code/kimmik123/all-about-linear-regression\">notebooks on certain topics</a> that I hope can guide and inspire beginners throughout their journey in Kaggle and beyond. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14269f02",
   "metadata": {
    "papermill": {
     "duration": 0.017955,
     "end_time": "2023-01-23T06:50:49.674754",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.656799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For today's notebook, I thought of presenting the concept of backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc37650",
   "metadata": {
    "papermill": {
     "duration": 0.015985,
     "end_time": "2023-01-23T06:50:49.707068",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.691083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### With the exponential rise of AI, taking on the forms of chat-bots, AI artists and even Chat GPT which seems to be inching closer to taking over the seemingly omnipotent Google search engine, humanity is constantly fascinated by it. \n",
    "### However, as you can see from my previous notebooks, I'm an avid believer in learning the foundations, and in this case, the foundation of Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223ad2a",
   "metadata": {
    "papermill": {
     "duration": 0.01563,
     "end_time": "2023-01-23T06:50:49.739163",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.723533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color: lightblue;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:black;\n",
    "          text-align: center;\n",
    "          font-size:20px;\">\n",
    "    <b>Backpropagation</b>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbb98c",
   "metadata": {
    "papermill": {
     "duration": 0.016182,
     "end_time": "2023-01-23T06:50:49.771590",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.755408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b46ac",
   "metadata": {
    "papermill": {
     "duration": 0.015385,
     "end_time": "2023-01-23T06:50:49.802918",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.787533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The brief overview of that backpropagation is can be summarised as such.\n",
    "### Backpropagation is a fundamental yet paramount algorithm for supervised learning of Neural Networks.\n",
    "### It achieves this by adjusting and optimizing weights to 'learn'.\n",
    "### The key principle used to 'learn' can be filtered to these two important elements.\n",
    "### **Gradient Descent** and **Chain Rule of Calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae5b71",
   "metadata": {
    "papermill": {
     "duration": 0.015564,
     "end_time": "2023-01-23T06:50:49.834732",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.819168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f8251",
   "metadata": {
    "papermill": {
     "duration": 0.016413,
     "end_time": "2023-01-23T06:50:49.866409",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.849996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### So how was it introduced to humanity? \n",
    "### The techinque of backpropagation itself was invented in the 1970s but it was not until 1986, when the paper titled \"Learning representations by backpropagating errors' was published. \n",
    "### This shed light on the utility of backpropagation in the field of machine learning.\n",
    "### It enabled models to help automatically discover good, non-trivial internal representations, in this case, the hidden nodes and layers.\n",
    "### Before this, researchers had to hand pick features, decide which were the important ones, frequently with the help of domain experts.\n",
    "### However, with this technique, it paved the road to a more efficient algorithm that achieved reduced time and cost restraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa29ca",
   "metadata": {
    "papermill": {
     "duration": 0.016481,
     "end_time": "2023-01-23T06:50:49.899592",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.883111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now that I have talked about the background and rough overview of this technique, let's dive into the numerical analysis shall we?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5021d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T14:18:53.509075Z",
     "iopub.status.busy": "2023-01-17T14:18:53.508590Z",
     "iopub.status.idle": "2023-01-17T14:18:53.517684Z",
     "shell.execute_reply": "2023-01-17T14:18:53.515595Z",
     "shell.execute_reply.started": "2023-01-17T14:18:53.509034Z"
    },
    "papermill": {
     "duration": 0.014989,
     "end_time": "2023-01-23T06:50:49.929930",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.914941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> \n",
    "<img align=middle src=\"https://images.unsplash.com/photo-1531637987854-a331f29be8fd?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1081&q=80\" style=\"width:50%;height:50%;margin:auto;\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8c5cc",
   "metadata": {
    "papermill": {
     "duration": 0.014888,
     "end_time": "2023-01-23T06:50:49.960174",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.945286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bfeff",
   "metadata": {
    "papermill": {
     "duration": 0.017949,
     "end_time": "2023-01-23T06:50:49.993447",
     "exception": false,
     "start_time": "2023-01-23T06:50:49.975498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now as I said, to understand backpropagation, we first need to walk through what the chain rule is.\n",
    "### For those of you who took some basic calculus, this might sound simple to you so feel free to skip ahead. \n",
    "### But for those who maybe forgot or are unfamiliar, allow me to introduce you to our dear friend, the **chain rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69660e",
   "metadata": {
    "papermill": {
     "duration": 0.014803,
     "end_time": "2023-01-23T06:50:50.024376",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.009573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The chain rule is a rule in calculus that allows us to find the derivative of a composite function. \n",
    "### A composite function is a function that is made up of two or more other functions. \n",
    "### The chain rule states that if we have a function y = f(u) and u = g(x), then the derivative of y with respect to x is given by **dy/dx = dy/du * du/dx**. \n",
    "### In other words, the derivative of the outer function f with respect to the inner function g, multiplied by the derivative of the inner function g with respect to x. \n",
    "### This rule can be extended to multiple levels of composition as well, such as three equations, which we shall see in just a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc15ff",
   "metadata": {
    "papermill": {
     "duration": 0.014859,
     "end_time": "2023-01-23T06:50:50.054344",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.039485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928119ca",
   "metadata": {
    "papermill": {
     "duration": 0.015008,
     "end_time": "2023-01-23T06:50:50.085199",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.070191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### To understand it simpler, just think of it as 'chaining' multiple derivative equations to derive at the ultimate derivative equation that we wanted from the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bac2c",
   "metadata": {
    "papermill": {
     "duration": 0.015417,
     "end_time": "2023-01-23T06:50:50.116247",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.100830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d103e9",
   "metadata": {
    "papermill": {
     "duration": 0.016083,
     "end_time": "2023-01-23T06:50:50.148207",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.132124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### We finally arrived at the core of today's notebook, the inner working of the backpropagation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ccda8",
   "metadata": {
    "papermill": {
     "duration": 0.014814,
     "end_time": "2023-01-23T06:50:50.179177",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.164363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### To begin, let us define our cost function as such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13da24c",
   "metadata": {
    "papermill": {
     "duration": 0.015168,
     "end_time": "2023-01-23T06:50:50.209866",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.194698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE C_0 = (a^L - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0af16f",
   "metadata": {
    "papermill": {
     "duration": 0.016684,
     "end_time": "2023-01-23T06:50:50.242709",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.226025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### where $a^L$ represents the value of the node in the last layer and $y$ as the value of the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b391cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T14:31:09.279712Z",
     "iopub.status.busy": "2023-01-17T14:31:09.279273Z",
     "iopub.status.idle": "2023-01-17T14:31:09.284665Z",
     "shell.execute_reply": "2023-01-17T14:31:09.283583Z",
     "shell.execute_reply.started": "2023-01-17T14:31:09.279674Z"
    },
    "papermill": {
     "duration": 0.01503,
     "end_time": "2023-01-23T06:50:50.273395",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.258365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### In this example, let us assume there are just single node in every layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa86d05",
   "metadata": {
    "papermill": {
     "duration": 0.015254,
     "end_time": "2023-01-23T06:50:50.304595",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.289341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Furthermore, we can represent $a^L$ as such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50913e1",
   "metadata": {
    "papermill": {
     "duration": 0.015631,
     "end_time": "2023-01-23T06:50:50.335347",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.319716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE a^L = \\sigma(z^L) $$\n",
    "$$\\LARGE z^L = (w^{L}a^{L-1} + b^L) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fd914",
   "metadata": {
    "papermill": {
     "duration": 0.01547,
     "end_time": "2023-01-23T06:50:50.366413",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.350943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### where $\\sigma$ represents the activation function, $w^L$ represents the weight of layer L, $b^L$ represents the bias of the layer L and $a^{L-1}$ represents the value of the node at layer $L-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecf6af",
   "metadata": {
    "papermill": {
     "duration": 0.015445,
     "end_time": "2023-01-23T06:50:50.396937",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.381492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Keep in mind that the whole point of backpropagation is to take the output error and propagate it backwards to adjust the weights and biases accordingly to obtain the lowest error possible.\n",
    "### To minimize the error $C_0$, most of us would think of calculus, thanks to the word 'minimize'. \n",
    "### If you did, you are right! \n",
    "### We would have to use the idea of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2c617",
   "metadata": {
    "papermill": {
     "duration": 0.014818,
     "end_time": "2023-01-23T06:50:50.427282",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.412464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### To do so, we would first have to check how the cost function would change **according** to the relevant variables, and in this case let us use the weight $w^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70396257",
   "metadata": {
    "papermill": {
     "duration": 0.01498,
     "end_time": "2023-01-23T06:50:50.457815",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.442835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C_0}}{\\partial{w^L}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc112886",
   "metadata": {
    "papermill": {
     "duration": 0.014987,
     "end_time": "2023-01-23T06:50:50.487820",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.472833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This would mathematically translate to the rate of change of the cost respect to the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272eccd9",
   "metadata": {
    "papermill": {
     "duration": 0.015372,
     "end_time": "2023-01-23T06:50:50.518396",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.503024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now let us utilize the chain rule we learned above to break this equation down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e941f98",
   "metadata": {
    "papermill": {
     "duration": 0.014656,
     "end_time": "2023-01-23T06:50:50.548219",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.533563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C_0}}{\\partial{w^L}} = \\frac{\\partial{z^L}}{\\partial{w^L}} \\frac{\\partial{a^L}}{\\partial{z^L}} \\frac{\\partial{C_0}}{\\partial{a^L}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d414b00",
   "metadata": {
    "papermill": {
     "duration": 0.014746,
     "end_time": "2023-01-23T06:50:50.578017",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.563271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Breaking it down as such, it might look more compilcated than it was and you might be asking\n",
    "<div style=\"display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color: lightblue;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "<p style=\"padding: 10px;\n",
    "              color:black;\n",
    "          text-align:center\">\n",
    "    What is the point of applying the chain rule to the partial derivative?\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "### Allow me to show you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3811fa",
   "metadata": {
    "papermill": {
     "duration": 0.015327,
     "end_time": "2023-01-23T06:50:50.609548",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.594221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C_0}}{\\partial{a^L}} = 2(a^L - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6533c5",
   "metadata": {
    "papermill": {
     "duration": 0.014727,
     "end_time": "2023-01-23T06:50:50.639378",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.624651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{a^L}}{\\partial{z^L}} = \\sigma'(z^L)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd43d2",
   "metadata": {
    "papermill": {
     "duration": 0.014885,
     "end_time": "2023-01-23T06:50:50.669246",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.654361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This derivative depends on what the activation function is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ae7ad",
   "metadata": {
    "papermill": {
     "duration": 0.015108,
     "end_time": "2023-01-23T06:50:50.699358",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.684250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{z^L}}{\\partial{w^L}} = a^{L-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129abca",
   "metadata": {
    "papermill": {
     "duration": 0.015663,
     "end_time": "2023-01-23T06:50:50.731217",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.715554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Merging all of them together would yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc054ee",
   "metadata": {
    "papermill": {
     "duration": 0.014851,
     "end_time": "2023-01-23T06:50:50.762078",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.747227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C_0}}{\\partial{w^L}} = a^{L-1} \\cdot \\sigma'(z^L) \\cdot 2(a^L - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca907e7",
   "metadata": {
    "papermill": {
     "duration": 0.014792,
     "end_time": "2023-01-23T06:50:50.791860",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.777068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### By utilizing the chain rule, we were able to break down the derivative we were looking for into what we can easily manipulate and calculate.\n",
    "### Now we only have to take into account a few variables to calculate the partial derivative. \n",
    "### However, in practical usage, we would have more than one training example for which the cost is represented as $C$ instead of $C_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545ea99",
   "metadata": {
    "papermill": {
     "duration": 0.01565,
     "end_time": "2023-01-23T06:50:50.822608",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.806958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C}}{\\partial{w^L}} = \\frac{1}{n} \\sum_{i=0}^{n-1} \\frac{\\partial{C_i}}{\\partial{w^L}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b05816",
   "metadata": {
    "papermill": {
     "duration": 0.014728,
     "end_time": "2023-01-23T06:50:50.852440",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.837712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### And remember, in practical usage, we would also have more than a single layer of neural network, so we would have to acount for $\\frac{\\partial{C}}{\\partial{w^1}}$ all the way to $\\frac{\\partial{C}}{\\partial{w^L}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7053423",
   "metadata": {
    "papermill": {
     "duration": 0.014756,
     "end_time": "2023-01-23T06:50:50.882414",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.867658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now this is for a single layer, with a single training example. \n",
    "### Some of you may ask, well how would the equation change in the real world, with multiple layers and numerous training examples?\n",
    "### Actually, not much changes!\n",
    "### It is just a mere increase in the number of subscripts and indexes as we have to keep track of multiple neurons in a single layer and training for multiple examples.\n",
    "### If you wish for a more visual and clearer explanation, check out the nearly legendary video from 3Blue1Brown [here](https://www.youtube.com/watch?v=tIeHLnjs5U8&ab_channel=3Blue1Brown)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975ca42",
   "metadata": {
    "papermill": {
     "duration": 0.015108,
     "end_time": "2023-01-23T06:50:50.912593",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.897485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### And now with this partial derivative of cost with respect to the weights, we can adjust that specific weight with proportion to the adjusted learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9324c40",
   "metadata": {
    "papermill": {
     "duration": 0.015683,
     "end_time": "2023-01-23T06:50:50.944308",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.928625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now that was all the theoretical portion.\n",
    "### I hoped I was clear and simple enough. \n",
    "### If it wasn't, please feel free to leave a comment about any corrections, queries or improvements you can think of. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a4354",
   "metadata": {
    "papermill": {
     "duration": 0.014969,
     "end_time": "2023-01-23T06:50:50.974680",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.959711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Before we finish off, I would like to show you how this theory gets used practically, as I normally do with my other notebooks. \n",
    "### So let us jump into an example of using backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b077ce",
   "metadata": {
    "papermill": {
     "duration": 0.0152,
     "end_time": "2023-01-23T06:50:51.005465",
     "exception": false,
     "start_time": "2023-01-23T06:50:50.990265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%;text-align: center;\"> \n",
    "<img align=middle src=\"https://images.unsplash.com/photo-1531879251-3da65dd78c99?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=687&q=80\" style=\"width:50%;height:50%;margin:auto;\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1275aee",
   "metadata": {
    "papermill": {
     "duration": 0.015261,
     "end_time": "2023-01-23T06:50:51.036318",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.021057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Coding up a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46711760",
   "metadata": {
    "papermill": {
     "duration": 0.015917,
     "end_time": "2023-01-23T06:50:51.068318",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.052401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For our network right here, we would be tackling the classification problem of [Wheat Seeds dataset](https://archive.ics.uci.edu/ml/datasets/seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c1639f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.102342Z",
     "iopub.status.busy": "2023-01-23T06:50:51.101918Z",
     "iopub.status.idle": "2023-01-23T06:50:51.150731Z",
     "shell.execute_reply": "2023-01-23T06:50:51.149390Z"
    },
    "papermill": {
     "duration": 0.068486,
     "end_time": "2023-01-23T06:50:51.153145",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.084659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>P</th>\n",
       "      <th>C</th>\n",
       "      <th>LK</th>\n",
       "      <th>WK</th>\n",
       "      <th>A_Coef</th>\n",
       "      <th>LKG</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.631</td>\n",
       "      <td>4.870</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.325</td>\n",
       "      <td>5.003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.315</td>\n",
       "      <td>5.056</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.598</td>\n",
       "      <td>5.044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A      P       C     LK     WK  A_Coef    LKG  target\n",
       "0    15.26  14.84  0.8710  5.763  3.312   2.221  5.220       0\n",
       "1    14.88  14.57  0.8811  5.554  3.333   1.018  4.956       0\n",
       "2    14.29  14.09  0.9050  5.291  3.337   2.699  4.825       0\n",
       "3    13.84  13.94  0.8955  5.324  3.379   2.259  4.805       0\n",
       "4    16.14  14.99  0.9034  5.658  3.562   1.355  5.175       0\n",
       "..     ...    ...     ...    ...    ...     ...    ...     ...\n",
       "205  12.19  13.20  0.8783  5.137  2.981   3.631  4.870       2\n",
       "206  11.23  12.88  0.8511  5.140  2.795   4.325  5.003       2\n",
       "207  13.20  13.66  0.8883  5.236  3.232   8.315  5.056       2\n",
       "208  11.84  13.21  0.8521  5.175  2.836   3.598  5.044       2\n",
       "209  12.30  13.34  0.8684  5.243  2.974   5.637  5.063       2\n",
       "\n",
       "[210 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/seed-from-uci/Seed_Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86231f69",
   "metadata": {
    "papermill": {
     "duration": 0.015262,
     "end_time": "2023-01-23T06:50:51.184109",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.168847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For this dataset, we have 7 feature variables in total, along with three possible targets, 0, 1 and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b10ae8",
   "metadata": {
    "papermill": {
     "duration": 0.015495,
     "end_time": "2023-01-23T06:50:51.215846",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.200351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize Neural Network\n",
    "### For this network, we will keep it very simple and just have a input layer, one hidden layer, and a final output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9461133a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.249302Z",
     "iopub.status.busy": "2023-01-23T06:50:51.248481Z",
     "iopub.status.idle": "2023-01-23T06:50:51.256325Z",
     "shell.execute_reply": "2023-01-23T06:50:51.254878Z"
    },
    "papermill": {
     "duration": 0.02729,
     "end_time": "2023-01-23T06:50:51.258869",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.231579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf1b80",
   "metadata": {
    "papermill": {
     "duration": 0.015912,
     "end_time": "2023-01-23T06:50:51.291028",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.275116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The parameters are as intuitive as it sounds, which are just number of neurons at the respect layers.\n",
    "### The weights for now are being randomly produced and it will only be trained using backpropagation later on.\n",
    "### We add one to the weights to allocate space for the bias for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee188b83",
   "metadata": {
    "papermill": {
     "duration": 0.015235,
     "end_time": "2023-01-23T06:50:51.322237",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.307002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forward Propagation\n",
    "### Forward propagation is the process of passing input data through a neural network to generate output. \n",
    "### It involves computing the dot product of the input data with the weights of the connections between the layers, passing the result through an activation function, and repeating this process for each layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faefb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T05:59:13.509922Z",
     "iopub.status.busy": "2023-01-18T05:59:13.509552Z",
     "iopub.status.idle": "2023-01-18T05:59:13.515297Z",
     "shell.execute_reply": "2023-01-18T05:59:13.514015Z",
     "shell.execute_reply.started": "2023-01-18T05:59:13.509891Z"
    },
    "papermill": {
     "duration": 0.015941,
     "end_time": "2023-01-23T06:50:51.354399",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.338458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Let's first calculate the result that the weights and biases provide to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8bbcc",
   "metadata": {
    "papermill": {
     "duration": 0.015952,
     "end_time": "2023-01-23T06:50:51.386681",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.370729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$ \\LARGE activation = \\sum_{i=0}^{n}({w_{i}^{L} \\cdot a_{i}^{L-1}}) + b^L$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebee7481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.420554Z",
     "iopub.status.busy": "2023-01-23T06:50:51.420153Z",
     "iopub.status.idle": "2023-01-23T06:50:51.425843Z",
     "shell.execute_reply": "2023-01-23T06:50:51.424614Z"
    },
    "papermill": {
     "duration": 0.02543,
     "end_time": "2023-01-23T06:50:51.428121",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.402691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# activation = sum(weight_i * input_i) + bias\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1] # Bias\n",
    "    for i in range(len(weights)-1): # Don't multiply the bias \n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87090b28",
   "metadata": {
    "papermill": {
     "duration": 0.015298,
     "end_time": "2023-01-23T06:50:51.458979",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.443681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Then we would need to apply the activation function.\n",
    "### Now the activation function should be something non-linear and definitely differentiable. \n",
    "### If not, we will face the problem of a disappearing gradient and backpropagation would no longer be viable.\n",
    "### The tradition is to use the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "### The main advantage of this function is that it can be [beautifully and easily differentiated](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186d421",
   "metadata": {
    "papermill": {
     "duration": 0.015548,
     "end_time": "2023-01-23T06:50:51.490159",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.474611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b5f191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.526572Z",
     "iopub.status.busy": "2023-01-23T06:50:51.526145Z",
     "iopub.status.idle": "2023-01-23T06:50:51.530955Z",
     "shell.execute_reply": "2023-01-23T06:50:51.529994Z"
    },
    "papermill": {
     "duration": 0.026593,
     "end_time": "2023-01-23T06:50:51.533007",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.506414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + math.exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df9811f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.567613Z",
     "iopub.status.busy": "2023-01-23T06:50:51.567181Z",
     "iopub.status.idle": "2023-01-23T06:50:51.573842Z",
     "shell.execute_reply": "2023-01-23T06:50:51.572793Z"
    },
    "papermill": {
     "duration": 0.027551,
     "end_time": "2023-01-23T06:50:51.576256",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.548705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer: # Loop for every single neuron \n",
    "            activation = activate(neuron['weights'], inputs) # Read below if confused here\n",
    "            neuron['output'] = transfer(activation) # Save the output to that dictionary\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99954902",
   "metadata": {
    "papermill": {
     "duration": 0.019134,
     "end_time": "2023-01-23T06:50:51.612452",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.593318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For those of you potentially confused at the part where the activate function takes in increasing sizes of the variable 'input', you don't have to worry because if you look at the activate function, you will notice it just loops over the length-1 of the weights, so it doesn't matter how long the inputs gets. \n",
    "### It is helpful to us as it stores the result of this layer's output to be used for the next layer's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d975649",
   "metadata": {
    "papermill": {
     "duration": 0.020825,
     "end_time": "2023-01-23T06:50:51.650273",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.629448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68c9e4",
   "metadata": {
    "papermill": {
     "duration": 0.018613,
     "end_time": "2023-01-23T06:50:51.690232",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.671619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now it is time for backpropagation. \n",
    "### The forward propagation would give us output, but it is merely based on random weights.\n",
    "### Now we have to go through the iterative process of backpropagation to adjust our weights to minimize our model's error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132103c",
   "metadata": {
    "papermill": {
     "duration": 0.016091,
     "end_time": "2023-01-23T06:50:51.721959",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.705868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### As we saw earlier, the end product what the chain rule has presented to us looks as such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea128cb9",
   "metadata": {
    "papermill": {
     "duration": 0.016025,
     "end_time": "2023-01-23T06:50:51.754843",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.738818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial{C_0}}{\\partial{w^L}} = a^{L-1} \\cdot \\sigma'(z^L) \\cdot 2(a^L - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8b9d3",
   "metadata": {
    "papermill": {
     "duration": 0.015286,
     "end_time": "2023-01-23T06:50:51.785759",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.770473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The middle term might be what everyone is worried about. \n",
    "### I have mentioned that that depends on what the activation function is, and in this case it is the sigmoid function. \n",
    "### As I have said it is easy to differentiate, allow me to show you what the derivative looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3fee4",
   "metadata": {
    "papermill": {
     "duration": 0.015355,
     "end_time": "2023-01-23T06:50:51.817181",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.801826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$$\\LARGE \\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x)(1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9238dc",
   "metadata": {
    "papermill": {
     "duration": 0.015284,
     "end_time": "2023-01-23T06:50:51.848480",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.833196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now do you see why I said the sigoid function is easy to differentiate?\n",
    "### The derivative of the sigmoid function can simple be expressed as the result of the function itself multiplied by one minus that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a208aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.882553Z",
     "iopub.status.busy": "2023-01-23T06:50:51.881356Z",
     "iopub.status.idle": "2023-01-23T06:50:51.887432Z",
     "shell.execute_reply": "2023-01-23T06:50:51.885986Z"
    },
    "papermill": {
     "duration": 0.025648,
     "end_time": "2023-01-23T06:50:51.890196",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.864548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25195d",
   "metadata": {
    "papermill": {
     "duration": 0.015415,
     "end_time": "2023-01-23T06:50:51.921409",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.905994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### A clear explanation and more detailed derivation could be found [here](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e) for those of you who want to know how that differentiation was achieved or if you could not clearly see the link to the final expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f23d7",
   "metadata": {
    "papermill": {
     "duration": 0.015609,
     "end_time": "2023-01-23T06:50:51.953453",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.937844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now, let us finally create a function to propagate the error backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99ce8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:51.986369Z",
     "iopub.status.busy": "2023-01-23T06:50:51.985881Z",
     "iopub.status.idle": "2023-01-23T06:50:51.995717Z",
     "shell.execute_reply": "2023-01-23T06:50:51.994411Z"
    },
    "papermill": {
     "duration": 0.029662,
     "end_time": "2023-01-23T06:50:51.998780",
     "exception": false,
     "start_time": "2023-01-23T06:50:51.969118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = []\n",
    "        if i != len(network)-1: # output layer \n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else: # hidden layers inbetween \n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ff9a1",
   "metadata": {
    "papermill": {
     "duration": 0.016136,
     "end_time": "2023-01-23T06:50:52.031608",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.015472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The error values inbetween are stored as 'delta' in the neurons themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed227f48",
   "metadata": {
    "papermill": {
     "duration": 0.016218,
     "end_time": "2023-01-23T06:50:52.063579",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.047361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873a1f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T07:49:37.768190Z",
     "iopub.status.busy": "2023-01-18T07:49:37.767818Z",
     "iopub.status.idle": "2023-01-18T07:49:37.773015Z",
     "shell.execute_reply": "2023-01-18T07:49:37.771521Z",
     "shell.execute_reply.started": "2023-01-18T07:49:37.768160Z"
    },
    "papermill": {
     "duration": 0.01535,
     "end_time": "2023-01-23T06:50:52.095109",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.079759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now that we have the basic functions all written down, let us train our network.\n",
    "### Training a deep neural network requires three things. \n",
    "### 1. Forward propagating the input data to obtain a result \n",
    "### 2. Backpropagating the error \n",
    "### 3. Updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56396212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:52.131101Z",
     "iopub.status.busy": "2023-01-23T06:50:52.130671Z",
     "iopub.status.idle": "2023-01-23T06:50:52.138029Z",
     "shell.execute_reply": "2023-01-23T06:50:52.136826Z"
    },
    "papermill": {
     "duration": 0.028507,
     "end_time": "2023-01-23T06:50:52.140411",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.111904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta'] # Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9444fd64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:52.177254Z",
     "iopub.status.busy": "2023-01-23T06:50:52.176822Z",
     "iopub.status.idle": "2023-01-23T06:50:52.187142Z",
     "shell.execute_reply": "2023-01-23T06:50:52.185494Z"
    },
    "papermill": {
     "duration": 0.031166,
     "end_time": "2023-01-23T06:50:52.189818",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.158652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "    print('error=%.3f' % (sum_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a687fb8",
   "metadata": {
    "papermill": {
     "duration": 0.01587,
     "end_time": "2023-01-23T06:50:52.222850",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.206980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22681883",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:52.260610Z",
     "iopub.status.busy": "2023-01-23T06:50:52.259656Z",
     "iopub.status.idle": "2023-01-23T06:50:52.275809Z",
     "shell.execute_reply": "2023-01-23T06:50:52.274762Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038678,
     "end_time": "2023-01-23T06:50:52.278284",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.239606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "\treturn stats\n",
    "\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)-1):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "dataset = load_csv(\"/kaggle/input/seeds-dataset/seeds_dataset.txt\")\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93cfd5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T06:50:52.314434Z",
     "iopub.status.busy": "2023-01-23T06:50:52.313207Z",
     "iopub.status.idle": "2023-01-23T06:51:22.756147Z",
     "shell.execute_reply": "2023-01-23T06:51:22.754871Z"
    },
    "papermill": {
     "duration": 30.466436,
     "end_time": "2023-01-23T06:51:22.761122",
     "exception": false,
     "start_time": "2023-01-23T06:50:52.294686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:30<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error=209.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 200, n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33f80b",
   "metadata": {
    "papermill": {
     "duration": 0.027697,
     "end_time": "2023-01-23T06:51:22.817406",
     "exception": false,
     "start_time": "2023-01-23T06:51:22.789709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24a320",
   "metadata": {
    "papermill": {
     "duration": 0.027128,
     "end_time": "2023-01-23T06:51:22.871989",
     "exception": false,
     "start_time": "2023-01-23T06:51:22.844861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now this was meant to be a theory focused exposition of the backpropagation algorithm but I wanted to show you guys how that seemingly complex algorithm could be implemented easily. \n",
    "### The work was heavily inspired from [my favorite website](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/), so feel free to check it out if you want to dive into more of these topics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52703062",
   "metadata": {
    "papermill": {
     "duration": 0.028086,
     "end_time": "2023-01-23T06:51:22.928139",
     "exception": false,
     "start_time": "2023-01-23T06:51:22.900053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### I hope with this notebook, I leave you with a bigger sense of appreciation for this whole backpropagation algorithm and it's mathematical foundations. \n",
    "### It really is not too difficult to wrap your head around this theory once you know some basic calculus and linear algebra if you want to expand onto a very large neural network. \n",
    "### I believe understanding AI and Machine Learning is extremely important for all, especially as we are living in the time of evolving AI. \n",
    "### Trust me, the consequences AI can bring upon humanity can be either a blessing or a downfall, and for me, helping me write this notebook ðŸ¤“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99d530",
   "metadata": {
    "papermill": {
     "duration": 0.027266,
     "end_time": "2023-01-23T06:51:22.983372",
     "exception": false,
     "start_time": "2023-01-23T06:51:22.956106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### As always thank you for sticking through till here! \n",
    "### Do check out my other notebooks if you found this one helpful and stay tuned for more. Cheers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea4059",
   "metadata": {
    "papermill": {
     "duration": 0.02756,
     "end_time": "2023-01-23T06:51:23.039467",
     "exception": false,
     "start_time": "2023-01-23T06:51:23.011907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a53ac",
   "metadata": {
    "papermill": {
     "duration": 0.027135,
     "end_time": "2023-01-23T06:51:23.094361",
     "exception": false,
     "start_time": "2023-01-23T06:51:23.067226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "* https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "* https://openai.com/blog/chatgpt/ (Cheeky)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.00594,
   "end_time": "2023-01-23T06:51:23.845406",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-23T06:50:39.839466",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
